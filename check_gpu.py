#!/usr/bin/env python3
"""
GPUç¯å¢ƒæ£€æŸ¥è„šæœ¬
ç”¨äºè¯Šæ–­PyTorchå’ŒCUDAç¯å¢ƒ
"""
import sys
import subprocess

def check_python_packages():
    """æ£€æŸ¥PythonåŒ…ç‰ˆæœ¬"""
    print("=" * 60)
    print("ğŸ Python ç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)
    
    packages = ['torch', 'transformers', 'datasets']
    for package in packages:
        try:
            result = subprocess.run([sys.executable, '-m', 'pip', 'show', package], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                version = next((line.split(': ')[1] for line in lines if line.startswith('Version: ')), 'Unknown')
                print(f"âœ… {package}: {version}")
            else:
                print(f"âŒ {package}: æœªå®‰è£…")
        except Exception as e:
            print(f"âŒ {package}: æ£€æŸ¥å¤±è´¥ - {e}")

def check_cuda_installation():
    """æ£€æŸ¥CUDAå®‰è£…"""
    print("\n" + "=" * 60)
    print("ğŸ”§ CUDA ç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)
    
    # æ£€æŸ¥nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… nvidia-smi å¯ç”¨")
            print(result.stdout)
        else:
            print("âŒ nvidia-smi ä¸å¯ç”¨")
    except FileNotFoundError:
        print("âŒ nvidia-smi æœªæ‰¾åˆ° - å¯èƒ½æœªå®‰è£…NVIDIAé©±åŠ¨")
    
    # æ£€æŸ¥nvcc
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… nvcc å¯ç”¨")
            lines = result.stdout.strip().split('\n')
            version_line = next((line for line in lines if 'release' in line), 'Unknown')
            print(f"   {version_line}")
        else:
            print("âŒ nvcc ä¸å¯ç”¨")
    except FileNotFoundError:
        print("âŒ nvcc æœªæ‰¾åˆ° - å¯èƒ½æœªå®‰è£…CUDA Toolkit")

def check_pytorch_cuda():
    """æ£€æŸ¥PyTorch CUDAæ”¯æŒ"""
    print("\n" + "=" * 60)
    print("ğŸ”¥ PyTorch CUDA æ£€æŸ¥")
    print("=" * 60)
    
    try:
        import torch
        print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… CUDA å¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"âœ… CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"âœ… cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
            print(f"âœ… GPU æ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
            
            # æµ‹è¯•GPUè®¡ç®—
            print("\nğŸ§ª GPU è®¡ç®—æµ‹è¯•:")
            try:
                x = torch.randn(1000, 1000).cuda()
                y = torch.randn(1000, 1000).cuda()
                z = torch.mm(x, y)
                print("âœ… GPU çŸ©é˜µä¹˜æ³•æµ‹è¯•é€šè¿‡")
            except Exception as e:
                print(f"âŒ GPU è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
                
        else:
            print("âŒ CUDA ä¸å¯ç”¨")
            print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
            print("   - PyTorch æœªå®‰è£… CUDA ç‰ˆæœ¬")
            print("   - NVIDIA é©±åŠ¨æœªå®‰è£…æˆ–ç‰ˆæœ¬ä¸åŒ¹é…")
            print("   - CUDA Toolkit æœªå®‰è£…")
            
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…")

def check_training_environment():
    """æ£€æŸ¥è®­ç»ƒç¯å¢ƒ"""
    print("\n" + "=" * 60)
    print("ğŸ¯ è®­ç»ƒç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)
    
    try:
        import torch
        from transformers import T5ForConditionalGeneration, AutoTokenizer
        
        print("âœ… Transformers åº“å¯ç”¨")
        
        # æµ‹è¯•æ¨¡å‹åŠ è½½
        print("ğŸ“¦ æµ‹è¯•æ¨¡å‹åŠ è½½...")
        tokenizer = AutoTokenizer.from_pretrained("t5-small")
        model = T5ForConditionalGeneration.from_pretrained("t5-small")
        print("âœ… T5 æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•GPUç§»åŠ¨
        if torch.cuda.is_available():
            print("ğŸš€ æµ‹è¯•æ¨¡å‹GPUç§»åŠ¨...")
            model = model.cuda()
            print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°: {next(model.parameters()).device}")
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            print("âš¡ æµ‹è¯•GPUå‰å‘ä¼ æ’­...")
            input_ids = torch.randint(0, 1000, (1, 10)).cuda()
            with torch.no_grad():
                outputs = model(input_ids)
            print("âœ… GPU å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
        else:
            print("âš ï¸  è·³è¿‡GPUæµ‹è¯• (CUDAä¸å¯ç”¨)")
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒç¯å¢ƒæ£€æŸ¥å¤±è´¥: {e}")

def main():
    print("ğŸ” 24ç‚¹è®­ç»ƒé¡¹ç›® GPU ç¯å¢ƒè¯Šæ–­")
    print("=" * 60)
    
    check_python_packages()
    check_cuda_installation()
    check_pytorch_cuda()
    check_training_environment()
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ æ€»ç»“å’Œå»ºè®®")
    print("=" * 60)
    
    try:
        import torch
        if torch.cuda.is_available():
            print("ğŸ‰ æ‚¨çš„ç¯å¢ƒæ”¯æŒGPUè®­ç»ƒ!")
            print("ğŸ’¡ å»ºè®®ä½¿ç”¨ train_improved.py è¿›è¡Œè®­ç»ƒ")
        else:
            print("âš ï¸  æ‚¨çš„ç¯å¢ƒä¸æ”¯æŒGPUè®­ç»ƒ")
            print("ğŸ’¡ å»ºè®®:")
            print("   1. å®‰è£…NVIDIAé©±åŠ¨")
            print("   2. å®‰è£…CUDA Toolkit")
            print("   3. å®‰è£…PyTorch CUDAç‰ˆæœ¬: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…PyTorch")

if __name__ == "__main__":
    main() 